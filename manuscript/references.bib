Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{John2012,
abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm. Keywords},
annote = {@Nora: QRP, RK, OS},
author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
doi = {10.1177/0956797611430953},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/John, Loewenstein, Prelec{\_}2012.pdf:pdf},
isbn = {0956-7976},
journal = {Psychological Science},
number = {5},
pages = {524--532},
title = {{Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}},
url = {https://doi.org/10.1177/0956797611430953},
volume = {23},
year = {2012}
}
@article{Gu2012,
abstract = {In this paper, we make the case for an open science in technology enhanced learning (TEL). Open science means opening up the research process by making all of its outcomes, and the way in which these outcomes were achieved, publicly available on the World Wide Web. In our vision, the adoption of open science instruments provides a set of solid and sustainable ways to connect the disjoint communities in TEL. Furthermore, we envision that researchers in TEL would be able to reproduce the results from any paper using the instruments of open science. Therefore, we introduce the concept of open methodology, which stands for sharing the methodological details of the evaluation provided, and the tools used for data collection and analysis. We discuss the potential benefits, but also the issues of an open science, and conclude with a set of recommendations for implementing open science in TEL.},
annote = {@Nora: OS},
author = {G{\"{u}}, N.A. and Reinhardt, Wolfgang and Leony, Derick and Kraker, Peter and nter Beham},
doi = {10.1504/ijtel.2011.045454},
issn = {1753-5255},
journal = {International Journal of Technology Enhanced Learning},
number = {6},
pages = {643},
title = {{The case for an open science in technology enhanced learning}},
volume = {3},
year = {2012}
}
@article{Johnson2017a,
abstract = {Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97{\%} of the original studies, statistical significance was achieved in only 36{\%} of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90{\%} of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Johnson, Valen E. and Payne, Richard D. and Wang, Tianying and Asher, Alex and Mandal, Soutrik},
doi = {10.1080/01621459.2016.1240079},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Null hypothesis significance test,Posterior model probability,Publication bias,Reproducibility,Significance test},
number = {517},
pages = {1--10},
pmid = {26315443},
title = {{On the Reproducibility of Psychological Science}},
volume = {112},
year = {2017}
}
@article{Klein2018,
abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54{\%}) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50{\%}) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25{\%}) of the replications yielded effect sizes larger than the original ones, and 21 (75{\%}) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 1...},
author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and Batra, Rishtee and Berkics, Mih{\'{a}}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'{e}}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and {Dalla Rosa}, Anna and Davis, William E. and de Bruijn, Maaike and {De Schutter}, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'{A}}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'{o}}mez, {\'{A}}ngel and Gonz{\'{a}}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, {\AA}se H. and Jim{\'{e}}nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v{z}}evi{\'{c}}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"{e}}l and Lazarevi{\'{c}}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedovi{\'{c}}, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'{e}}lix and {Lee Nichols}, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'{a}}bor and Osowiecka, Malgorzata and Packard, Grant and P{\'{e}}rez-S{\'{a}}nchez, Rolando and Petrovi{\'{c}}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"{o}}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'{u}}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'{a}}squez- Echeverr{\'{i}}a}, Alejandro and {Ann Vaughn}, Leigh and V{\'{a}}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
doi = {10.1177/2515245918810225},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs//Klein et al.{\_}2018.pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {Registered Report,cognitive psychology,culture,individual differences,many labs,meta-analysis,open data,open materials,preregistered,replication,sampling effects,situational effects,social psychology},
mendeley-tags = {many labs,replication},
month = {dec},
number = {4},
pages = {443--490},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Many Labs 2: Investigating Variation in Replicability Across Samples and Settings}},
url = {http://journals.sagepub.com/doi/10.1177/2515245918810225},
volume = {1},
year = {2018}
}
@book{Cohen1988,
address = {Hillsdale NJ: Erlbaum},
author = {Cohen, J.},
title = {{Statistical power analysis for the behavioral sciences}},
year = {1988}
}
@article{Landis2016,
author = {Landis, Ronald S. and Rupp, Deborah E. and Banks, George C. and Woznyj, Haley M. and Rogelberg, Steven G.},
doi = {10.1007/s10869-016-9456-7},
file = {::},
issn = {0889-3268},
journal = {Journal of Business and Psychology},
keywords = {Ethics,Philosophy of science,Questionable research practices QRPs,Research methodology,Research methods,of science {\'{a}} ethics,questionable research practices qrps,research methodology {\'{a}} philosophy,{\'{a}}},
number = {3},
pages = {323--338},
publisher = {Springer US},
title = {{Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly}},
volume = {31},
year = {2016}
}
@article{Krishna2018a,
abstract = {Although questionable research practices (QRPs) and p-hacking have received attention in recent years, little research has focused on their prevalence and acceptance in students. Students are the researchers of the future and will represent the field in the future. Therefore, they should not be learning to use and accept QRPs, which would reduce their ability to produce and evaluate meaningful research. 207 psychology students and fresh graduates provided self-report data on the prevalence and predictors of QRPs. Attitudes towards QRPs, belief that significant results constitute better science or lead to better grades, motivation, and stress levels were predictors. Furthermore, we assessed perceived supervisor attitudes towards QRPs as an important predictive factor. The results were in line with estimates of QRP prevalence from academia. The best predictor of QRP use was students' QRP attitudes. Perceived supervisor attitudes exerted both a direct and indirect effect via student attitudes. Motivation to write a good thesis was a protective factor, whereas stress had no effect. Students in this sample did not subscribe to beliefs that significant results were better for science or their grades. Such beliefs further did not impact QRP attitudes or use in this sample. Finally, students engaged in more QRPs pertaining to reporting and analysis than those pertaining to study design. We conclude that supervisors have an important function in shaping students' attitudes towards QRPs and can improve their research practices by motivating them well. Furthermore, this research provides some impetus towards identifying predictors of QRP use in academia.},
author = {Krishna, Anand and Peter, Sebastian M.},
doi = {10.1371/journal.pone.0203470},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Krishna, Peter{\_}2018.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
title = {{Questionable research practices in student final theses – Prevalence, attitudes, and the role of the supervisor's perceived attitudes}},
volume = {13},
year = {2018}
}
@article{Agnoli2017a,
abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88{\%}) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Albiero, Paolo and Cubelli, Roberto},
doi = {10.1371/journal.pone.0172792},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Agnoli et al.{\_}2017.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pmid = {28296929},
title = {{Questionable research practices among Italian research psychologists}},
volume = {12},
year = {2017}
}
@article{Simmons2018a,
abstract = {This invited paper describes how we came to write an article called "False-Positive Psychology."},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/1745691617698146},
file = {::},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {p-hacking,preregistration,replicability,research methods},
number = {2},
pages = {255--259},
title = {{False-Positive Citations}},
volume = {13},
year = {2018}
}
@article{Sijtsma2016,
abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices (QRPs). Deliberate or not, QRPs usually have a deteriorating effect on the quality and the credibility of research results. QRPs must be revealed but prevention of QRPs is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one's methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-015-9446-0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sijtsma{\_}2016.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {data fraud,hiring a methodologist/statistician,public availability of data,questionable research practices,replication crisis,solutions},
mendeley-tags = {questionable research practices,replication crisis,solutions},
month = {mar},
number = {1},
pages = {1--15},
pmid = {25820980},
title = {{Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right}},
url = {http://link.springer.com/10.1007/s11336-015-9446-0},
volume = {81},
year = {2016}
}
@article{Wicherts2016a,
abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
author = {Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Augusteijn, Hilde E.M. and Bakker, Marjan and van Aert, Robbie C.M. and van Assen, Marcel A.L.M.},
doi = {10.3389/fpsyg.2016.01832},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Wicherts et al.{\_}2016.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Bias,Experimental design,P-hacking,Questionable research practices,Research methods education,Significance chasing,Significance testing},
number = {NOV},
pages = {1--12},
title = {{Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid P-hacking}},
volume = {7},
year = {2016}
}
@article{Matthes2015,
abstract = {Questionable research practices (QRPs) pose a major threat to any scientific discipline. This article analyzes QRPs with a content analysis of more than three decades of published experimental research in four flagship communication journals: Journal of Communication, Communication Research, Journalism {\&} Mass Communication Quarterly,and Media Psychology. Findings reveal indications of small and insufficiently justified sample sizes, a lack of reported effect sizes, an indiscriminate removal of cases and items, an increasing inflation of p-values directly below p {\textless} .05, and a rising share of verified (as opposed to falsified) hypotheses. Implications for authors, reviewers, and editors are discussed.},
author = {Matthes, J{\"{o}}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'{e}}e and Adam, Karoline},
doi = {10.1080/19312458.2015.1096334},
file = {::},
issn = {19312466},
journal = {Communication Methods and Measures},
number = {4},
pages = {193--207},
title = {{Questionable Research Practices in Experimental Communication Research: A Systematic Analysis From 1980 to 2013}},
volume = {9},
year = {2015}
}
@article{Fiedler2016,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Fiedler, Schwarz{\_}2016.pdf:pdf},
isbn = {8224406113},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,questionable research practices,replication crisis,research methods,research practices,survey methodology},
mendeley-tags = {questionable research practices,replication crisis},
month = {jan},
number = {1},
pages = {45--52},
pmid = {25869851},
title = {{Questionable Research Practices Revisited}},
url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
volume = {7},
year = {2016}
}
@article{Fiedler2016a,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Fiedler, Schwarz{\_}2016.pdf:pdf},
issn = {19485514},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,research methods,research practices,survey methodology},
number = {1},
pages = {45--52},
title = {{Questionable Research Practices Revisited}},
volume = {7},
year = {2016}
}
@article{Simmons2011,
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
archivePrefix = {arXiv},
arxivId = {2021},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
eprint = {2021},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Simmons, Nelson, Simonsohn{\_}2011.pdf:pdf},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,false positives,methodology,methods,motivated reasoning,p hacking,publication,replication crisis},
mendeley-tags = {false positives,methods,p hacking,replication crisis},
number = {11},
pages = {1359--1366},
pmid = {22006061},
title = {{False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant}},
volume = {22},
year = {2011}
}
@article{OpenScienceCollaboration2015,
abstract = {INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {::},
journal = {Science},
number = {6251},
pages = {aac4716},
title = {{Estimating the reproducibility of psychological science}},
url = {http://science.sciencemag.org/content/349/6251/aac4716?casa{\_}token=7AWGYeOunWQAAAAA:7pBsRnNCE5IVS8ECLfGRYsgcGEV{\_}7xXs97it2Y3ihTEsQN9ixtTbBUbsao-RUrucVy4nbqjJHlkvOg},
volume = {349},
year = {2015}
}
@article{Sijtsma2016a,
abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices (QRPs). Deliberate or not, QRPs usually have a deteriorating effect on the quality and the credibility of research results. QRPs must be revealed but prevention of QRPs is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one's methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-015-9446-0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sijtsma{\_}2016.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {data fraud,hiring a methodologist/statistician,public availability of data,questionable research practices},
number = {1},
pages = {1--15},
pmid = {25820980},
title = {{Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right}},
volume = {81},
year = {2016}
}
@article{Abston2016,
author = {Abston, Kristie A. and White, Charles D. and Bennett, Andrew A. and Batchelor, John H. and Adkins, Cheryl L. and Banks, George C. and Whelpley, Christopher E. and Pollack, Jeffrey M. and O'Boyle, Ernest H.},
doi = {10.1177/0149206315619011},
file = {::},
issn = {0149-2063},
journal = {Journal of Management},
number = {1},
pages = {5--20},
title = {{Questions About Questionable Research Practices in the Field of Management}},
volume = {42},
year = {2016}
}
@article{Fanelli2009,
abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, "cooking" of data, etc... Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97{\%} (N = 7, 95{\%}CI: 0.86-4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once--a serious form of misconduct by any standard--and up to 33.7{\%} admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12{\%} (N = 12, 95{\%} CI: 9.91-19.72) for falsification, and up to 72{\%} for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words "falsification" or "fabrication", and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
author = {Fanelli, Daniele},
doi = {10.1371/journal.pone.0005738},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
title = {{How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data}},
volume = {4},
year = {2009}
}
