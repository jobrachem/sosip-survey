Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Nosek2018a,
author = {Nosek, Brian A and Ebersole, Charles R and Dehaven, Alexander C and Mellor, David T},
doi = {10.1073/pnas.1708274114},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Nosek et al.{\_}2018(2).pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {11},
pages = {2600--2606},
title = {{The preregistration revolution}},
volume = {115},
year = {2018}
}
@article{Wicherts2016a,
abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
author = {Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Augusteijn, Hilde E.M. and Bakker, Marjan and van Aert, Robbie C.M. and van Assen, Marcel A.L.M.},
doi = {10.3389/fpsyg.2016.01832},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Wicherts et al.{\_}2016.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Bias,Experimental design,P-hacking,Questionable research practices,Research methods education,Significance chasing,Significance testing},
number = {NOV},
pages = {1--12},
title = {{Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid P-hacking}},
volume = {7},
year = {2016}
}
@article{Bakker2012,
abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96{\%} of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {\textless} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
archivePrefix = {arXiv},
arxivId = {hep-ph/0006269v1},
author = {Bakker, Marjan and van Dijk, Annette and Wicherts, Jelte M.},
doi = {10.1177/1745691612459060},
eprint = {0006269v1},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Bakker, van Dijk, Wicherts{\_}2012.pdf:pdf},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {false positives,p hacking,power,publication bias,replication,replication crisis,sample size},
mendeley-tags = {p hacking,publication bias,replication crisis},
number = {6},
pages = {543--554},
pmid = {26168111},
primaryClass = {hep-ph},
title = {{The rules of the game called psychological science}},
volume = {7},
year = {2012}
}
@article{Arslan2019,
abstract = {Open-source software improves the reproducibility of scientific research. Because existing open-source tools often do not offer dedicated support for longitudinal data collection on phones and computers, we built formr, a study framework that enables researchers to conduct both simple surveys and more intricate studies. With automated email and text message reminders that can be sent according to any schedule, longitudinal and experience-sampling studies become easy to implement. By integrating a web-based application programming interface for the statistical pro- gramming language R via OpenCPU, formr allows researchers to use a familiar programming language to enable complex features. These can range from adaptive testing, to graphical and interactive feedback, to integration with non-survey data sources such as self-trackers or online social network data. Here we showcase three studies created in formr: a study of couples with dyadic feedback; a longitudinal study over months, which included social networks and peer and partner ratings; and a diary study with daily invitations sent out by text message and email and extensive feedback on intraindividual patterns. Keywords},
author = {Arslan, Ruben C. and Walther, Matthias P. and Tata, Cyril S.},
doi = {10.3758/s13428-019-01236-y},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Arslan, Walther, Tata{\_}2019.pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Feedback,Online,R,Study,Survey,Web},
publisher = {Behavior Research Methods},
title = {{Formr: a study framework allowing for automated feedback generation and complex longitudinal experience-sampling studies using R}},
year = {2019}
}
@article{Abston2016,
author = {Abston, Kristie A. and White, Charles D. and Bennett, Andrew A. and Batchelor, John H. and Adkins, Cheryl L. and Banks, George C. and Whelpley, Christopher E. and Pollack, Jeffrey M. and O'Boyle, Ernest H.},
doi = {10.1177/0149206315619011},
file = {::},
issn = {0149-2063},
journal = {Journal of Management},
number = {1},
pages = {5--20},
title = {{Questions about questionable research practices in the field of management}},
volume = {42},
year = {2016}
}
@book{Lewandowsky2011,
author = {Lewandowsky, Stephan and Farrell, Simon},
doi = {10.4135/9781483349428},
publisher = {SAGE},
title = {{Computational modeling in cognition: principles and practice}},
year = {2011}
}
@article{Banks2016,
author = {Banks, George C. and Rogelberg, Steven G. and Woznyj, Haley M. and Landis, Ronald S. and Rupp, Deborah E.},
doi = {10.1007/s10869-016-9456-7},
file = {::},
issn = {0889-3268},
journal = {Journal of Business and Psychology},
keywords = {Ethics,Philosophy of science,Questionable research practices QRPs,Research methodology,Research methods,of science {\'{a}} ethics,questionable research practices qrps,research methodology {\'{a}} philosophy,{\'{a}}},
number = {3},
pages = {323--338},
publisher = {Springer US},
title = {{Editorial: Evidence on questionable research practices: the good, the bad, and the ugly}},
volume = {31},
year = {2016}
}
@article{Devezer2019,
abstract = {Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists' choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.},
archivePrefix = {arXiv},
arxivId = {1803.10118},
author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
doi = {10.1371/journal.pone.0216125},
eprint = {1803.10118},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Devezer et al.{\_}2019.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
pages = {1--23},
title = {{Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity}},
volume = {14},
year = {2019}
}
@article{Fiedler2016,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Fiedler, Schwarz{\_}2016.pdf:pdf},
isbn = {8224406113},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,questionable research practices,replication crisis,research methods,research practices,survey methodology},
mendeley-tags = {questionable research practices,replication crisis},
month = {jan},
number = {1},
pages = {45--52},
pmid = {25869851},
title = {{Questionable research practices revisited}},
url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
volume = {7},
year = {2016}
}
@article{Sarafoglou2020,
author = {Sarafoglou, Alexandra and Hoogeveen, Suzanne and Matzke, Dora and Wagenmakers, Eric-Jan},
doi = {10.1177/1475725719858807},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sarafoglou et al.{\_}2020.pdf:pdf},
journal = {Psychology, Learning {\&} Teaching},
keywords = {education,meta-science,open science,replication crisis},
number = {1},
pages = {45----59},
title = {{Teaching good research practices: protocol of a research master course}},
volume = {19},
year = {2020}
}
@book{Cohen1988,
address = {Hillsdale NJ: Erlbaum},
author = {Cohen, J.},
title = {{Statistical power analysis for the behavioral sciences}},
year = {1988}
}
@article{Dienes2018,
abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in motivating researchers to conclude that H1 is supported better than H0, and the other way round, that H0 is better supported than H1. The next four, however, show that the methods will also often disagree. In these cases, the aim of the paper will be to motivate the sensible evidential conclusion, and then see which approach matches those intuitions. Specifically, it is shown that a high-powered non-significant result is consistent with no evidence for H0 over H1 worth mentioning, which a Bayes factor can show, and, conversely, that a low-powered non-significant result is consistent with substantial evidence for H0 over H1, again indicated by Bayesian analyses. The fourth study illustrates that a high-powered significant result may not amount to any evidence for H1 over H0, matching the Bayesian conclusion. Finally, the fifth study illustrates that different theories can be evidentially supported to different degrees by the same data; a fact that P-values cannot reflect but Bayes factors can. It is argued that appropriate conclusions match the Bayesian inferences, but not those based on significance testing, where they disagree.},
author = {Dienes, Zoltan and Mclatchie, Neil},
doi = {10.3758/s13423-017-1266-z},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Dienes, Mclatchie{\_}2018.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayes factor,Bayesian statistics,Power,Significance testing,Statistics},
number = {1},
pages = {207--218},
pmid = {28353065},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Four reasons to prefer Bayesian analyses over significance testing}},
volume = {25},
year = {2018}
}
@article{Bates2015,
author = {Bates, Douglas and M{\"{a}}chler, Martin and Bolker, Ben and Walker, Steve},
doi = {10.18637/jss.v067.i01},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--48},
title = {{Fitting linear mixed-effects models using {\{}lme4{\}}}},
volume = {67},
year = {2015}
}
@article{Ulrich2016,
author = {Ulrich, Rolf and Erdfelder, Edgar and Deutsch, Roland and Strau{\ss}, Bernhard and Br{\"{u}}ggemann, Anne and Hannover, Bettina and Tuschen-Caffier, Brunna and Kirschbaum, Clemens and Blickle, Gerhard and M{\"{o}}ller, Jens and Rief, Winfried},
doi = {10.1026/0033-3042/a000296},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jul},
number = {3},
pages = {163--174},
title = {{Inflation von falsch-positiven Befunden in der psychologischen Forschung}},
url = {https://econtent.hogrefe.com/doi/10.1026/0033-3042/a000296},
volume = {67},
year = {2016}
}
@misc{DeutscheGesellschaftfurPsychologie2005,
author = {{Deutsche Gesellschaft f{\"{u}}r Psychologie}},
title = {{Empfehlungen der DGPs zur Einrichtung von B.Sc.-M.Sc.-Studieng{\"{a}}ngen in Psychologie an den Universit{\"{a}}ten}},
url = {https://www.dgps.de/index.php?id=143{\&}tx{\_}ttnews{\%}5Btt{\_}news{\%}5D=993{\&}cHash=03097507021fb333802bbeaf76963033},
year = {2005}
}
@article{Klein2018,
abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54{\%}) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50{\%}) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25{\%}) of the replications yielded effect sizes larger than the original ones, and 21 (75{\%}) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 1...},
author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and Batra, Rishtee and Berkics, Mih{\'{a}}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'{e}}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and {Dalla Rosa}, Anna and Davis, William E. and de Bruijn, Maaike and {De Schutter}, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'{A}}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'{o}}mez, {\'{A}}ngel and Gonz{\'{a}}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, {\AA}se H. and Jim{\'{e}}nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v{z}}evi{\'{c}}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"{e}}l and Lazarevi{\'{c}}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedovi{\'{c}}, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'{e}}lix and {Lee Nichols}, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'{a}}bor and Osowiecka, Malgorzata and Packard, Grant and P{\'{e}}rez-S{\'{a}}nchez, Rolando and Petrovi{\'{c}}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"{o}}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'{u}}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'{a}}squez- Echeverr{\'{i}}a}, Alejandro and {Ann Vaughn}, Leigh and V{\'{a}}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
doi = {10.1177/2515245918810225},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Klein et al.{\_}2018(2).pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {Registered Report,cognitive psychology,culture,individual differences,many labs,meta-analysis,open data,open materials,preregistered,replication,sampling effects,situational effects,social psychology},
mendeley-tags = {many labs,replication},
month = {dec},
number = {4},
pages = {443--490},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Many Labs 2: investigating variation in replicability across samples and settings}},
url = {http://journals.sagepub.com/doi/10.1177/2515245918810225},
volume = {1},
year = {2018}
}
@techreport{Psyfako2018,
author = {{Konferenzrat der Psychologie-Fachschaften-Konferenz}},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Konferenzrat der Psychologie-Fachschaften-Konferenz{\_}2018.pdf:pdf},
title = {{Positionspapier der Psychologie-Fachschaften-Konferenz (PsyFaKo) zum Thema „Replikationskrise und Open Science in der Psychologie“}},
url = {https://psyfako.org/index.php/positionspapiere/},
year = {2018}
}
@incollection{Wagenmakers2017a,
author = {Wagenmakers, Eric-Jan and Verhagen, Josine and Ly, Alexander and Matzke, Dora and Steingroever, Helen and Rouder, Jeff N. and Morey, Richard},
booktitle = {Psychological science under scrutiny: recent challenges and proposed solutions},
editor = {Lilienfeld, Scott O. and Waldman, I.},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Wagenmakers et al.{\_}2017.pdf:pdf},
issn = {16641078},
keywords = {Data testing,Logic,Modus tollens,P-value,Reductio argument,Statistics},
publisher = {John Wiley and Sons},
title = {{The need for Bayesian hypothesis testing in psychological science}},
year = {2017}
}
@article{Stanley2014,
abstract = {Failures to replicate published psychological research findings have contributed to a "crisis of confidence." Several reasons for these failures have been proposed, the most notable being questionable research practices and data fraud. We examine replication from a different perspective and illustrate that current intuitive expectations for replication are unreasonable. We used computer simulations to create thousands of ideal replications, with the same participants, wherein the only difference across replications was random measurement error. In the first set of simulations, study results differed substantially across replications as a result of measurement error alone. This raises questions about how researchers should interpret failed replication attempts, given the large impact that even modest amounts of measurement error can have on observed associations. In the second set of simulations, we illustrated the difficulties that researchers face when trying to interpret and replicate a published finding. We also assessed the relative importance of both sampling error and measurement error in producing variability in replications. Conventionally, replication attempts are viewed through the lens of verifying or falsifying published findings. We suggest that this is a flawed perspective and that researchers should adjust their expectations concerning replications and shift to a meta-analytic mind-set. {\textcopyright} The Author(s) 2014.},
author = {Stanley, David J. and Spence, Jeffrey R.},
doi = {10.1177/1745691614528518},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Stanley, Spence{\_}2014.pdf:pdf},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {individual differences,meta-analysis,methodology,reliability,replication,scientific},
number = {3},
pages = {305--318},
title = {{Expectations for replications: are yours realistic?}},
volume = {9},
year = {2014}
}
@manual{Arslan2014,
annote = {R package version 0.8.4},
author = {Arslan, Ruben},
title = {formr: formr survey framework},
year = {2014}
}
@misc{Barr2019,
abstract = {At Glasgow our PsyTeachR teaching team has successfully made the transition to teaching R across all undergraduate and postgraduate levels. In this workshop we will share our experiences, insights, and teaching materials. Our curriculum now emphasises certain essential ‘data science' graduate skills that have been overlooked in traditional approaches to teaching, including programming skills, data visualisation, data wrangling and reproducible reports. Students learn about probability and inference through data simulation as well as by working with real datasets. We have also instituted regular coursework across all levels, so that students build their skills through practical experience. We will discuss the challenges associated with implementing this new approach, including staff re-training, supporting student learning through online help sessions, and computer-assisted assessment to handle large volumes of coursework. The workshop is aimed at those interested in making the transition to teaching reproducible research in R at their own institutions, although we also would welcome participation from institutions where R is already a core part of the curriculum.},
author = {Barr, Dale and Woods, Heather Cleland and DeBruine, Lisa and Lai, Rebecca and McAleer, Phil and McNee, Shanon and Nordmann, Emily and Paterson, Helena and Stack, Niamh},
title = {{Redesigning methods curricula for reproducibility}},
url = {https://psyteachr.github.io/sips2019/},
urldate = {2019-08-30},
year = {2019}
}
@article{Mcshane2019,
author = {Mcshane, Blakeley B and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer},
doi = {10.1080/00031305.2018.1527253},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Mcshane et al.{\_}2019.pdf:pdf},
journal = {The American Statistician},
keywords = {null hypothesis significance,p -value,replication,sociology of science,testing},
number = {S1},
pages = {235--245},
title = {{Abandon statistical significance}},
volume = {73},
year = {2019}
}
@article{Nosek2014,
author = {Nosek, Brian A. and Lakens, Dani{\"{e}}l},
doi = {10.1027/1864-9335/a000192},
file = {::},
issn = {1864-9335},
journal = {Social Psychology},
month = {may},
number = {3},
pages = {137--141},
publisher = {Hogrefe Publishing},
title = {{Registered reports}},
url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000192},
volume = {45},
year = {2014}
}
@article{Wagge2019,
author = {Wagge, Jordan R and Brandt, Mark J and Lazarevic, Ljiljana B and Legate, Nicole and Christopherson, Cody and Wiggins, Brady and Grahe, Jon E},
doi = {10.3389/fpsyg.2019.00247},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Wagge et al.{\_}2019.pdf:pdf},
keywords = {open science,pedagogy,projects,psychology,publishing,replication,teaching,undergraduates},
title = {{Publishing research with undergraduate students via replication work: the collaborative replications and education project}},
volume = {10},
year = {2019}
}
@article{Simmons2011,
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
archivePrefix = {arXiv},
arxivId = {2021},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
eprint = {2021},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Simmons, Nelson, Simonsohn{\_}2011.pdf:pdf},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,false positives,methodology,methods,motivated reasoning,p hacking,publication,replication crisis},
mendeley-tags = {false positives,methods,p hacking,replication crisis},
number = {11},
pages = {1359--1366},
pmid = {22006061},
title = {{False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant}},
volume = {22},
year = {2011}
}
@article{Matthes2015,
abstract = {Questionable research practices (QRPs) pose a major threat to any scientific discipline. This article analyzes QRPs with a content analysis of more than three decades of published experimental research in four flagship communication journals: Journal of Communication, Communication Research, Journalism {\&} Mass Communication Quarterly,and Media Psychology. Findings reveal indications of small and insufficiently justified sample sizes, a lack of reported effect sizes, an indiscriminate removal of cases and items, an increasing inflation of p-values directly below p {\textless} .05, and a rising share of verified (as opposed to falsified) hypotheses. Implications for authors, reviewers, and editors are discussed.},
author = {Matthes, J{\"{o}}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'{e}}e and Adam, Karoline},
doi = {10.1080/19312458.2015.1096334},
file = {::},
issn = {19312466},
journal = {Communication Methods and Measures},
number = {4},
pages = {193--207},
title = {{Questionable research practices in experimental communication research: a systematic analysis from 1980 to 2013}},
volume = {9},
year = {2015}
}
@article{Button2013,
abstract = {Low-powered studies lead to overestimates of effect size and low reproducibility of results. In this Analysis article, Munaf{\`{o}} and colleagues show that the average statistical power of studies in the neurosciences is very low, discuss ethical implications of low-powered studies and provide recommendations to improve research practices.},
author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`{o}}, Marcus R.},
doi = {10.1038/nrn3475},
file = {::},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Molecular neuroscience},
month = {may},
number = {5},
pages = {365--376},
publisher = {Nature Publishing Group},
title = {{Power failure: why small sample size undermines the reliability of neuroscience}},
url = {http://www.nature.com/articles/nrn3475},
volume = {14},
year = {2013}
}
@article{Hammarberg2016,
author = {Hammarberg, K and Kirkman, M and Lacey, S De},
doi = {10.1093/humrep/dev334},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Hammarberg, Kirkman, Lacey{\_}2016.pdf:pdf},
number = {3},
pages = {498--501},
title = {{Qualitative research methods: when to use them and how to judge them}},
volume = {31},
year = {2016}
}
@article{Stroebe2014,
abstract = {There has been increasing criticism of the way psychologists conduct and analyze studies. These critiques as well as failures to replicate several high-profile studies have been used as justification to proclaim a "replication crisis" in psychology. Psychologists are encouraged to conduct more "exact" replications of published studies to assess the reproducibility of psychological research. This article argues that the alleged "crisis of replicability" is primarily due to an epistemological misunderstanding that emphasizes the phenomenon instead of its underlying mechanisms. As a consequence, a replicated phenomenon may not serve as a rigorous test of a theoretical hypothesis because identical operationalizations of variables in studies conducted at different times and with different subject populations might test different theoretical constructs. Therefore, we propose that for meaningful replications, attempts at reinstating the original circumstances are not sufficient. Instead, replicators must ascertain that conditions are realized that reflect the theoretical variable(s) manipulated (and/or measured) in the original study. {\textcopyright} The Author(s) 2013.},
author = {Stroebe, Wolfgang and Strack, Fritz},
doi = {10.1177/1745691613514450},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Stroebe, Strack{\_}2014.pdf:pdf},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {critical rationalism,epistemology,null findings,priming,replicability crisis,replication,scientific fraud},
number = {1},
pages = {59--71},
title = {{The alleged crisis and the illusion of exact replication}},
volume = {9},
year = {2014}
}
@article{Munafo2018,
abstract = {A different approach to experimentation, especially in complex environments},
author = {Munaf{\`{o}}, Marcus R. and {Davey Smith}, George},
doi = {10.1038/d41586-018-01023-3},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Munaf{\`{o}}, Davey Smith{\_}2018.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7689},
pages = {399--401},
pmid = {29368721},
title = {{Robust research needs many lines of evidence}},
volume = {553},
year = {2018}
}
@article{John2012,
abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm. Keywords},
annote = {@Nora: QRP, RK, OS},
author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
doi = {10.1177/0956797611430953},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/John, Loewenstein, Prelec{\_}2012.pdf:pdf},
isbn = {0956-7976},
journal = {Psychological Science},
number = {5},
pages = {524--532},
title = {{Measuring the prevalence of questionable research practices with incentives for truth telling}},
url = {https://doi.org/10.1177/0956797611430953},
volume = {23},
year = {2012}
}
@manual{RCoreTeam2019,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A language and environment for statistical computing}},
url = {https://www.r-project.org/},
year = {2019}
}
@article{Zwaan2017,
abstract = {Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A finding is elevated from single observation to scientific evidence when the procedures that were used to obtain it can be reproduced and the finding itself can be replicated. Recent replication attempts show that some high profile results – most notably in psychology, but in many other disciplines as well – cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.},
author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
doi = {10.1017/s0140525x17001972},
file = {::},
isbn = {0024-3590},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {psychological research,replication,reproducibility,research programs},
number = {2018},
pmid = {16961341},
title = {{Making replication mainstream}},
volume = {41},
year = {2017}
}
@article{Jekel2020,
author = {Jekel, Marc and Fiedler, Susann and Torras, Ramona Allstadt and Mischkowski, Dorothee and Dorrough, Angela Rachael and Glo, Andreas},
doi = {10.1177/1475725719868149},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Jekel et al.{\_}2020.pdf:pdf},
journal = {Psychology Learning {\&} Teaching},
number = {1},
pages = {91--106},
title = {{How to teach open science principles in the undergraduate curriculum - the Hagen cumulative science project}},
volume = {19},
year = {2020}
}
@article{Sarafoglou2019,
abstract = {The current crisis of confidence in psychological science has spurred on field-wide reforms to enhance transparency, reproducibility, and replicability. To solidify these reforms within the scientific community, student courses on open science practices are essential. Here we describe the content of our Research Master course “Good Research Practices” which we have designed and taught at the University of Amsterdam. Supported by Chambers' recent book The 7 Deadly Sins of Psychology, the course covered topics such as QRPs, the importance of direct and conceptual replication studies, preregistration, and the public sharing of data, code, and analysis plans. We adopted a pedagogical approach that: (a) reduced teacher-centered lectures to a minimum; (b) emphasized practical training on open science practices; and (c) encouraged students to engage in the ongoing discussions in the open science community on social media platforms.},
author = {Sarafoglou, Alexandra and Hoogeveen, Suzanne and Matzke, Dora and Wagenmakers, Eric-Jan},
doi = {10.1177/1475725719858807},
issn = {1475-7257},
journal = {Psychology Learning {\&} Teaching},
keywords = {Open science,Sara2019,education,meta-science,replication crisis},
mendeley-tags = {Sara2019},
month = {jul},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Teaching good research practices: protocol of a research master course}},
url = {http://journals.sagepub.com/doi/10.1177/1475725719858807},
year = {2019}
}
@article{Gu2012,
abstract = {In this paper, we make the case for an open science in technology enhanced learning (TEL). Open science means opening up the research process by making all of its outcomes, and the way in which these outcomes were achieved, publicly available on the World Wide Web. In our vision, the adoption of open science instruments provides a set of solid and sustainable ways to connect the disjoint communities in TEL. Furthermore, we envision that researchers in TEL would be able to reproduce the results from any paper using the instruments of open science. Therefore, we introduce the concept of open methodology, which stands for sharing the methodological details of the evaluation provided, and the tools used for data collection and analysis. We discuss the potential benefits, but also the issues of an open science, and conclude with a set of recommendations for implementing open science in TEL.},
annote = {@Nora: OS},
author = {G{\"{u}}, N.A. and Reinhardt, Wolfgang and Leony, Derick and Kraker, Peter and nter Beham},
doi = {10.1504/ijtel.2011.045454},
issn = {1753-5255},
journal = {International Journal of Technology Enhanced Learning},
number = {6},
pages = {643},
title = {{The case for an open science in technology enhanced learning}},
volume = {3},
year = {2012}
}
@article{Schoenbrodt2017,
author = {Sch{\"{o}}nbrodt, Felix and Gollwitzer, Mario and Abele-Brehm, Andrea},
doi = {10.1026/0033-3042/a000341},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {20--35},
title = {{Der Umgang mit Forschungsdaten im Fach Psychologie: Konkretisierung der DFG-Leitlinien}},
url = {https://econtent.hogrefe.com/doi/10.1026/0033-3042/a000341},
volume = {68},
year = {2017}
}
@article{Chopik2018,
abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining ho...},
author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
doi = {10.1177/0098628318762900},
issn = {0098-6283},
journal = {Teaching of Psychology},
keywords = {best practices,open-source materials,questionable research practices,replication,undergraduate pedagogy},
month = {apr},
number = {2},
pages = {158--163},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{How (and whether) to teach undergraduates about the replication crisis in psychological science}},
url = {http://journals.sagepub.com/doi/10.1177/0098628318762900},
volume = {45},
year = {2018}
}
@article{Camerer2018,
abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62{\%}) studies, and the effect size of the replications is on average about 50{\%} of the original effect size. Replicability varies between 12 (57{\%}) and 14 (67{\%}) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67{\%} in a Bayesian analysis. The relative effect size of true positives is estimated to be 71{\%}, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-th/9312160v1},
author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck Hua and Huber, J{\"{u}}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric Jan and Wu, Hang},
doi = {10.1038/s41562-018-0399-z},
eprint = {9312160v1},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Camerer et al.{\_}2018.pdf:pdf},
isbn = {2397-3374},
issn = {23973374},
journal = {Nature Human Behaviour},
keywords = {replication},
mendeley-tags = {replication},
number = {9},
pages = {637--644},
primaryClass = {arXiv:hep-th},
title = {{Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015}},
volume = {2},
year = {2018}
}
@article{Sijtsma2016a,
abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices (QRPs). Deliberate or not, QRPs usually have a deteriorating effect on the quality and the credibility of research results. QRPs must be revealed but prevention of QRPs is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one's methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-015-9446-0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sijtsma{\_}2016.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {data fraud,hiring a methodologist/statistician,public availability of data,questionable research practices},
number = {1},
pages = {1--15},
pmid = {25820980},
title = {{Playing with data—or how to discourage questionable research practices and stimulate researchers to do things right}},
volume = {81},
year = {2016}
}
@article{Cohen1994,
abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing -mechanical dichotomous decisions around a sacred .05 criterion—still persists. This article reviews the problems with this practice, including its near-universal misinterpretation ofp as the probability that Ho isfalse, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects Ho one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods is suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
author = {Cohen, Jacob},
doi = {10.1037/0003-066X.49.12.997},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Cohen{\_}1994.pdf:pdf},
journal = {American Psychologist},
number = {12},
pages = {997--1003},
title = {{The earth is round (p {\textless} .05)}},
volume = {49},
year = {1994}
}
@article{Krishna2018a,
abstract = {Although questionable research practices (QRPs) and p-hacking have received attention in recent years, little research has focused on their prevalence and acceptance in students. Students are the researchers of the future and will represent the field in the future. Therefore, they should not be learning to use and accept QRPs, which would reduce their ability to produce and evaluate meaningful research. 207 psychology students and fresh graduates provided self-report data on the prevalence and predictors of QRPs. Attitudes towards QRPs, belief that significant results constitute better science or lead to better grades, motivation, and stress levels were predictors. Furthermore, we assessed perceived supervisor attitudes towards QRPs as an important predictive factor. The results were in line with estimates of QRP prevalence from academia. The best predictor of QRP use was students' QRP attitudes. Perceived supervisor attitudes exerted both a direct and indirect effect via student attitudes. Motivation to write a good thesis was a protective factor, whereas stress had no effect. Students in this sample did not subscribe to beliefs that significant results were better for science or their grades. Such beliefs further did not impact QRP attitudes or use in this sample. Finally, students engaged in more QRPs pertaining to reporting and analysis than those pertaining to study design. We conclude that supervisors have an important function in shaping students' attitudes towards QRPs and can improve their research practices by motivating them well. Furthermore, this research provides some impetus towards identifying predictors of QRP use in academia.},
author = {Krishna, Anand and Peter, Sebastian M.},
doi = {10.1371/journal.pone.0203470},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Krishna, Peter{\_}2018.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
title = {{Questionable research practices in student final theses – Prevalence, attitudes, and the role of the supervisor's perceived attitudes}},
volume = {13},
year = {2018}
}
@article{Simmons2018a,
abstract = {This invited paper describes how we came to write an article called "False-Positive Psychology."},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/1745691617698146},
file = {::},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {p-hacking,preregistration,replicability,research methods},
number = {2},
pages = {255--259},
title = {{False-positive citations}},
volume = {13},
year = {2018}
}
@manual{Wickham2017,
annote = {R package version 1.2.1},
author = {Wickham, Hadley},
title = {{Tidyverse: easily install and load the 'tidyverse'}},
url = {https://cran.r-project.org/package=tidyverse},
year = {2017}
}
@article{Tackett2017,
abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has...},
author = {Tackett, Jennifer L. and Lilienfeld, Scott O. and Patrick, Christopher J. and Johnson, Sheri L. and Krueger, Robert F. and Miller, Joshua D. and Oltmanns, Thomas F. and Shrout, Patrick E.},
doi = {10.1177/1745691617690042},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {assessment,diagnosis,disorders,scientific methodology},
month = {sep},
number = {5},
pages = {742--756},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{It's time to broaden the replicability conversation: thoughts for and from clinical psychological science}},
url = {http://journals.sagepub.com/doi/10.1177/1745691617690042},
volume = {12},
year = {2017}
}
@article{Johnson2017a,
abstract = {Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97{\%} of the original studies, statistical significance was achieved in only 36{\%} of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90{\%} of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Johnson, Valen E. and Payne, Richard D. and Wang, Tianying and Asher, Alex and Mandal, Soutrik},
doi = {10.1080/01621459.2016.1240079},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Null hypothesis significance test,Posterior model probability,Publication bias,Reproducibility,Significance test},
number = {517},
pages = {1--10},
pmid = {26315443},
title = {{On the reproducibility of psychological science}},
volume = {112},
year = {2017}
}
@article{Amrhein2019,
author = {Amrhein, Valentin and Greenland, Sander and Mcshane, Blakeley B.},
doi = {10.1111/eci.13176},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Amrhein, Greenland, Mcshane{\_}2019.pdf:pdf},
journal = {European Journal of Clinical Investigation},
number = {12},
title = {{Statistical significance gives bias a free pass}},
volume = {49},
year = {2019}
}
@article{Glockner2018,
abstract = {Zusammenfassung. Die Replikationskrise innerhalb der Psychologie hat eine Diskussion {\"{u}}ber g{\"{a}}ngige Praktiken im Forschungsprozess und die beteiligten Institutionen ausgel{\"{o}}st. Wir stellen Ma{\ss}nahmen v...},
author = {Gl{\"{o}}ckner, Andreas and Fiedler, Susann and Renkewitz, Frank},
doi = {10.1026/0033-3042/a000384},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Gl{\"{o}}ckner, Fiedler, Renkewitz{\_}2018.pdf:pdf},
issn = {0033-3042},
journal = {Psychologische Rundschau},
keywords = {der herausforderung,erkenntnisfort-,methodology,open science,schritts und zur vermeidung,sich jede wissenschaftliche disziplin,theory databases,theory specification,von fehlentwicklungen muss,zur gew{\"{a}}hrleistung eines effizienten},
number = {1},
pages = {22--36},
title = {{Belastbare und effiziente Wissenschaft}},
volume = {69},
year = {2018}
}
@article{Gilbert2016a,
abstract = {A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.},
author = {Gilbert, Daniel T. and King, Gary and Pettigrew, Stephen and Wilson, Timothy D.},
doi = {10.1126/science.aad7243},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Gilbert et al.{\_}2016(2).pdf:pdf},
issn = {10959203},
journal = {Science},
number = {6277},
title = {{Comment on "Estimating the reproducibility of psychological science"}},
volume = {351},
year = {2016}
}
@article{Agnoli2017a,
abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88{\%}) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Albiero, Paolo and Cubelli, Roberto},
doi = {10.1371/journal.pone.0172792},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Agnoli et al.{\_}2017.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pmid = {28296929},
title = {{Questionable research practices among Italian research psychologists}},
volume = {12},
year = {2017}
}
@article{Chambers2013,
author = {Chambers, Christopher D.},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Chambers{\_}2013.pdf:pdf},
journal = {Cortex},
pages = {609--610},
title = {{Registered reports: a new publishing initiative at Cortex}},
volume = {49},
year = {2013}
}
@article{Edwards2017,
abstract = {Abstract Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics...},
author = {Edwards, Marc A. and Roy, Siddhartha},
doi = {10.1089/ees.2016.0223},
issn = {1557-9018},
journal = {Environmental Engineering Science},
keywords = {academic research,funding,misconduct,perverse incentives,scientific integrity},
month = {jan},
number = {1},
pages = {51--61},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{Academic research in the 21st century: maintaining scientific integrity in a climate of perverse incentives and hypercompetition}},
url = {http://www.liebertpub.com/doi/10.1089/ees.2016.0223},
volume = {34},
year = {2017}
}
@article{Fiedler2018,
abstract = {The Open Science Collaboration's 2015 report suggests that replication effect sizes in psychology are modest. However, closer inspection reveals serious problems. When plotting replication effects are against original effects, the regression trap is lurking: Expecting replication effects to be equally strong as original effects is logically unwarranted; they are inevitably subject to regressive shrinkage. To control for regression, the reliability of original and replication studies must be taken into account. Further problems arise from missing manipulation checks and sampling biases. Our critical comment highlights the need for replication science to live up to the same methodological scrutiny as other research.},
author = {Fiedler, Klaus and Prager, Johannes},
doi = {10.1080/01973533.2017.1421953},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Fiedler, Prager{\_}2018.pdf:pdf},
issn = {01973533},
journal = {Basic and Applied Social Psychology},
number = {3},
pages = {115--124},
publisher = {Taylor {\&} Francis},
title = {{The regression trap and other pitfalls of replication science—illustrated by the report of the Open Science Collaboration}},
url = {https://doi.org/10.1080/01973533.2017.1421953},
volume = {40},
year = {2018}
}
@article{Fanelli2009,
abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, "cooking" of data, etc... Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97{\%} (N = 7, 95{\%}CI: 0.86-4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once--a serious form of misconduct by any standard--and up to 33.7{\%} admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12{\%} (N = 12, 95{\%} CI: 9.91-19.72) for falsification, and up to 72{\%} for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words "falsification" or "fabrication", and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
author = {Fanelli, Daniele},
doi = {10.1371/journal.pone.0005738},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
title = {{How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data}},
volume = {4},
year = {2009}
}
@article{Kuznetsova2017,
author = {Kuznetsova, Alexandra and Brockhoff, Per B and Christensen, Rune H B},
doi = {10.18637/jss.v082.i13},
journal = {Journal of Statistical Software},
number = {13},
pages = {1--26},
title = {{{\{}lmerTest{\}} package: tests in linear mixed effects models}},
volume = {82},
year = {2017}
}
@misc{Orben2020,
author = {Orben, Amy},
title = {{Psychology as a robust science [Course Syllabus]}},
url = {https://www.amyorben.com/docs/syllabus.pdf},
year = {2020}
}
@article{Klauer2018,
author = {Klauer, Karl Christoph},
doi = {10.1026/0033-3042/a000385},
file = {::},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {1--2},
publisher = {Hogrefe Publishing Group},
title = {{Themenheft Replizierbarkeit}},
volume = {69},
year = {2018}
}
@article{OpenScienceCollaboration2015,
abstract = {INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {::},
journal = {Science},
number = {6251},
pages = {aac4716},
title = {{Estimating the reproducibility of psychological science}},
url = {http://science.sciencemag.org/content/349/6251/aac4716?casa{\_}token=7AWGYeOunWQAAAAA:7pBsRnNCE5IVS8ECLfGRYsgcGEV{\_}7xXs97it2Y3ihTEsQN9ixtTbBUbsao-RUrucVy4nbqjJHlkvOg},
volume = {349},
year = {2015}
}
@article{Schonbrodt2018,
abstract = {Zusammenfassung. Gro{\ss} angelegte Replikationsprojekte der letzten Jahre legen ein aus unserer Sicht beunruhigendes Ausma{\ss} an nicht-replizierbaren Befunden in der wissenschaftlichen Literatur nahe, sowohl in der Psychologie als auch in anderen Disziplinen. Basierend auf einer Analyse einiger Ursachen dieser Situation argumentieren wir, dass der Wandel hin zu einer offenen Wissenschaft (?Open Science?) eine Konsequenz aus der Glaubw{\"{u}}rdigkeitskrise sein muss. Wir pl{\"{a}}dieren f{\"{u}}r konkrete und machbare {\"{A}}nderungen in den Arbeitseinheiten und Instituten vor Ort, und zeigen exemplarisch, welche Schritte am Department Psychologie der Ludwig-Maximilians-Universit{\"{a}}t M{\"{u}}nchen umgesetzt wurden. Diese Schritte betreffen Anreizstrukturen, die Forschungskultur, die Lehre und die Verzahnung mit der Ethikkommission. Sie haben das Ziel, eine reproduzierbarere und glaubw{\"{u}}rdigere Forschung zu unterst{\"{u}}tzen, ohne unn{\"{o}}tige b{\"{u}}rokratische Belastungen zu erzeugen.},
author = {Sch{\"{o}}nbrodt, Felix D. and Maier, Markus and Heene, Moritz and B{\"{u}}hner, Markus},
doi = {10.1026/0033-3042/a000386},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sch{\"{o}}nbrodt et al.{\_}2018.pdf:pdf},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {37--44},
publisher = {Hogrefe Publishing Group},
title = {{Forschungstransparenz als hohes wissenschaftliches Gut st{\"{a}}rken}},
volume = {69},
year = {2018}
}
@article{Beck-Bomholdt1996,
author = {Beck-Bomholdt, H. P. and Dubben, H. H.},
doi = {10.1038/381730d0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Beck-Bomholdt, Dubben{\_}1996.pdf:pdf},
issn = {00280836},
journal = {Nature},
keywords = {data testing,logic,modus tollens,p -value,p-value,reductio argument,statistics},
number = {6585},
pages = {730},
title = {{Commentary: the need for Bayesian hypothesis testing in psychological science}},
volume = {381},
year = {1996}
}
@misc{Allaire2020,
author = {Allaire, J and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
title = {{Rmarkdown: dynamic documents for R}},
url = {https://github.com/rstudio/rmarkdown},
year = {2020}
}
@article{Nosek2020,
abstract = {Credibility of scientific claims is established with evidence for their replicability using new data. According to common understanding, replication is repeating a study's procedure and observing whether the prior finding recurs. This definition is intuitive, easy to apply, and incorrect. We propose that replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research. This definition reduces emphasis on operational characteristics of the study and increases emphasis on the interpretation of possible outcomes. The purpose of replication is to advance theory by confronting existing understanding with new evidence. Ironically, the value of replication may be strongest when existing understanding is weakest. Successful replication provides evidence of generalizability across the conditions that inevitably differ from the original study; Unsuccessful replication indicates that the reliability of the finding may be more constrained than recognized previously. Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.},
author = {Nosek, Brian A. and Errington, Timothy M.},
doi = {10.1371/journal.pbio.3000691},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Nosek, Errington{\_}2020.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
number = {3},
pages = {1--8},
pmid = {32218571},
title = {{What is replication?}},
url = {http://dx.doi.org/10.1371/journal.pbio.3000691},
volume = {18},
year = {2020}
}
@article{Earp2015,
abstract = {The (latest) “crisis in confidence” in social psychology has generated much heated discussion about the importance of replication, including how such replication should be carried out as well as interpreted by scholars in the field. What does it mean if a replication attempt “fails”—does it mean that the original results, or the theory that predicted them, have been falsified? And how should “failed” replications affect our belief in the validity of the original research? In this paper, we consider the “replication” debate from a historical and philosophical perspective, and provide a conceptual analysis of both replication and falsification as they pertain to this important discussion. Along the way, we introduce a Bayesian framework for assessing “failed” replications in terms of how they should affect our confidence in purported findings.},
author = {Earp, Brian D. and Trafimow, David},
doi = {10.3389/fpsyg.2015.00621},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Earp, Trafimow{\_}2015.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Falsification,Philosophy of science,Psychology,Replication,Social,crisis of replicability},
month = {may},
pages = {621},
publisher = {Frontiers},
title = {{Replication, falsification, and the crisis of confidence in social psychology}},
url = {http://www.frontiersin.org/Quantitative{\_}Psychology{\_}and{\_}Measurement/10.3389/fpsyg.2015.00621/abstract},
volume = {6},
year = {2015}
}
