Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Psyfako2018,
author = {{Konferenzrat der Psychologie-Fachschaften-Konferenz}},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Konferenzrat der Psychologie-Fachschaften-Konferenz{\_}2018.pdf:pdf},
title = {{Positionspapier der Psychologie-Fachschaften-Konferenz (PsyFaKo) zum Thema „Replikationskrise und Open Science in der Psychologie“}},
url = {https://psyfako.org/index.php/positionspapiere/},
year = {2018}
}
@article{Fiedler2016,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Fiedler, Schwarz{\_}2016.pdf:pdf},
isbn = {8224406113},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,questionable research practices,replication crisis,research methods,research practices,survey methodology},
mendeley-tags = {questionable research practices,replication crisis},
month = {jan},
number = {1},
pages = {45--52},
pmid = {25869851},
title = {{Questionable Research Practices Revisited}},
url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
volume = {7},
year = {2016}
}
@article{Schoenbrodt2017,
author = {Sch{\"{o}}nbrodt, Felix and Gollwitzer, Mario and Abele-Brehm, Andrea},
doi = {10.1026/0033-3042/a000341},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {20--35},
title = {{Der Umgang mit Forschungsdaten im Fach Psychologie: Konkretisierung der DFG-Leitlinien}},
url = {https://econtent.hogrefe.com/doi/10.1026/0033-3042/a000341},
volume = {68},
year = {2017}
}
@article{Zwaan2017,
abstract = {Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A finding is elevated from single observation to scientific evidence when the procedures that were used to obtain it can be reproduced and the finding itself can be replicated. Recent replication attempts show that some high profile results – most notably in psychology, but in many other disciplines as well – cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.},
author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
doi = {10.1017/s0140525x17001972},
file = {::},
isbn = {0024-3590},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {psychological research,replication,reproducibility,research programs},
number = {2018},
pmid = {16961341},
title = {{Making replication mainstream}},
volume = {41},
year = {2017}
}
@article{Fanelli2009,
abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, "cooking" of data, etc... Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97{\%} (N = 7, 95{\%}CI: 0.86-4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once--a serious form of misconduct by any standard--and up to 33.7{\%} admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12{\%} (N = 12, 95{\%} CI: 9.91-19.72) for falsification, and up to 72{\%} for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words "falsification" or "fabrication", and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
author = {Fanelli, Daniele},
doi = {10.1371/journal.pone.0005738},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
title = {{How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data}},
volume = {4},
year = {2009}
}
@article{Kuznetsova2017,
author = {Kuznetsova, Alexandra and Brockhoff, Per B and Christensen, Rune H B},
doi = {10.18637/jss.v082.i13},
journal = {Journal of Statistical Software},
number = {13},
pages = {1--26},
title = {{{\{}lmerTest{\}} Package: Tests in Linear Mixed Effects Models}},
volume = {82},
year = {2017}
}
@article{Button2013,
abstract = {Low-powered studies lead to overestimates of effect size and low reproducibility of results. In this Analysis article, Munaf{\`{o}} and colleagues show that the average statistical power of studies in the neurosciences is very low, discuss ethical implications of low-powered studies and provide recommendations to improve research practices.},
author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`{o}}, Marcus R.},
doi = {10.1038/nrn3475},
file = {::},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Molecular neuroscience},
month = {may},
number = {5},
pages = {365--376},
publisher = {Nature Publishing Group},
title = {{Power failure: why small sample size undermines the reliability of neuroscience}},
url = {http://www.nature.com/articles/nrn3475},
volume = {14},
year = {2013}
}
@article{Johnson2017a,
abstract = {Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97{\%} of the original studies, statistical significance was achieved in only 36{\%} of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90{\%} of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Johnson, Valen E. and Payne, Richard D. and Wang, Tianying and Asher, Alex and Mandal, Soutrik},
doi = {10.1080/01621459.2016.1240079},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Null hypothesis significance test,Posterior model probability,Publication bias,Reproducibility,Significance test},
number = {517},
pages = {1--10},
pmid = {26315443},
title = {{On the Reproducibility of Psychological Science}},
volume = {112},
year = {2017}
}
@article{Sarafoglou2019,
abstract = {The current crisis of confidence in psychological science has spurred on field-wide reforms to enhance transparency, reproducibility, and replicability. To solidify these reforms within the scientific community, student courses on open science practices are essential. Here we describe the content of our Research Master course “Good Research Practices” which we have designed and taught at the University of Amsterdam. Supported by Chambers' recent book The 7 Deadly Sins of Psychology, the course covered topics such as QRPs, the importance of direct and conceptual replication studies, preregistration, and the public sharing of data, code, and analysis plans. We adopted a pedagogical approach that: (a) reduced teacher-centered lectures to a minimum; (b) emphasized practical training on open science practices; and (c) encouraged students to engage in the ongoing discussions in the open science community on social media platforms.},
author = {Sarafoglou, Alexandra and Hoogeveen, Suzanne and Matzke, Dora and Wagenmakers, Eric-Jan},
doi = {10.1177/1475725719858807},
issn = {1475-7257},
journal = {Psychology Learning {\&} Teaching},
keywords = {Open science,Sara2019,education,meta-science,replication crisis},
mendeley-tags = {Sara2019},
month = {jul},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Teaching Good Research Practices: Protocol of a Research Master Course}},
url = {http://journals.sagepub.com/doi/10.1177/1475725719858807},
year = {2019}
}
@article{Earp2015,
abstract = {The (latest) “crisis in confidence” in social psychology has generated much heated discussion about the importance of replication, including how such replication should be carried out as well as interpreted by scholars in the field. What does it mean if a replication attempt “fails”—does it mean that the original results, or the theory that predicted them, have been falsified? And how should “failed” replications affect our belief in the validity of the original research? In this paper, we consider the “replication” debate from a historical and philosophical perspective, and provide a conceptual analysis of both replication and falsification as they pertain to this important discussion. Along the way, we introduce a Bayesian framework for assessing “failed” replications in terms of how they should affect our confidence in purported findings.},
author = {Earp, Brian D. and Trafimow, David},
doi = {10.3389/fpsyg.2015.00621},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Earp, Trafimow{\_}2015.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Falsification,Philosophy of science,Psychology,Replication,Social,crisis of replicability},
month = {may},
pages = {621},
publisher = {Frontiers},
title = {{Replication, falsification, and the crisis of confidence in social psychology}},
url = {http://www.frontiersin.org/Quantitative{\_}Psychology{\_}and{\_}Measurement/10.3389/fpsyg.2015.00621/abstract},
volume = {6},
year = {2015}
}
@article{Klauer2018,
author = {Klauer, Karl Christoph},
doi = {10.1026/0033-3042/a000385},
file = {::},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {1--2},
publisher = {Hogrefe Publishing Group},
title = {{Themenheft Replizierbarkeit}},
volume = {69},
year = {2018}
}
@article{Nosek2014,
author = {Nosek, Brian A. and Lakens, Dani{\"{e}}l},
doi = {10.1027/1864-9335/a000192},
file = {::},
issn = {1864-9335},
journal = {Social Psychology},
month = {may},
number = {3},
pages = {137--141},
publisher = {Hogrefe Publishing},
title = {{Registered Reports}},
url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000192},
volume = {45},
year = {2014}
}
@article{Arslan2019,
abstract = {Open-source software improves the reproducibility of scientific research. Because existing open-source tools often do not offer dedicated support for longitudinal data collection on phones and computers, we built formr, a study framework that enables researchers to conduct both simple surveys and more intricate studies. With automated email and text message reminders that can be sent according to any schedule, longitudinal and experience-sampling studies become easy to implement. By integrating a web-based application programming interface for the statistical pro- gramming language R via OpenCPU, formr allows researchers to use a familiar programming language to enable complex features. These can range from adaptive testing, to graphical and interactive feedback, to integration with non-survey data sources such as self-trackers or online social network data. Here we showcase three studies created in formr: a study of couples with dyadic feedback; a longitudinal study over months, which included social networks and peer and partner ratings; and a diary study with daily invitations sent out by text message and email and extensive feedback on intraindividual patterns. Keywords},
author = {Arslan, Ruben C. and Walther, Matthias P. and Tata, Cyril S.},
doi = {10.3758/s13428-019-01236-y},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Arslan, Walther, Tata{\_}2019.pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Feedback,Online,R,Study,Survey,Web},
publisher = {Behavior Research Methods},
title = {{formr: A study framework allowing for automated feedback generation and complex longitudinal experience-sampling studies using R}},
year = {2019}
}
@article{Klein2018,
abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54{\%}) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50{\%}) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25{\%}) of the replications yielded effect sizes larger than the original ones, and 21 (75{\%}) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 1...},
author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and Batra, Rishtee and Berkics, Mih{\'{a}}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'{e}}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and {Dalla Rosa}, Anna and Davis, William E. and de Bruijn, Maaike and {De Schutter}, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'{A}}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'{o}}mez, {\'{A}}ngel and Gonz{\'{a}}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, {\AA}se H. and Jim{\'{e}}nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v{z}}evi{\'{c}}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"{e}}l and Lazarevi{\'{c}}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedovi{\'{c}}, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'{e}}lix and {Lee Nichols}, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'{a}}bor and Osowiecka, Malgorzata and Packard, Grant and P{\'{e}}rez-S{\'{a}}nchez, Rolando and Petrovi{\'{c}}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"{o}}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'{u}}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'{a}}squez- Echeverr{\'{i}}a}, Alejandro and {Ann Vaughn}, Leigh and V{\'{a}}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
doi = {10.1177/2515245918810225},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs//Klein et al.{\_}2018.pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {Registered Report,cognitive psychology,culture,individual differences,many labs,meta-analysis,open data,open materials,preregistered,replication,sampling effects,situational effects,social psychology},
mendeley-tags = {many labs,replication},
month = {dec},
number = {4},
pages = {443--490},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Many Labs 2: Investigating Variation in Replicability Across Samples and Settings}},
url = {http://journals.sagepub.com/doi/10.1177/2515245918810225},
volume = {1},
year = {2018}
}
@article{Simmons2018a,
abstract = {This invited paper describes how we came to write an article called "False-Positive Psychology."},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/1745691617698146},
file = {::},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {p-hacking,preregistration,replicability,research methods},
number = {2},
pages = {255--259},
title = {{False-Positive Citations}},
volume = {13},
year = {2018}
}
@book{Cohen1988,
address = {Hillsdale NJ: Erlbaum},
author = {Cohen, J.},
title = {{Statistical power analysis for the behavioral sciences}},
year = {1988}
}
@article{Gu2012,
abstract = {In this paper, we make the case for an open science in technology enhanced learning (TEL). Open science means opening up the research process by making all of its outcomes, and the way in which these outcomes were achieved, publicly available on the World Wide Web. In our vision, the adoption of open science instruments provides a set of solid and sustainable ways to connect the disjoint communities in TEL. Furthermore, we envision that researchers in TEL would be able to reproduce the results from any paper using the instruments of open science. Therefore, we introduce the concept of open methodology, which stands for sharing the methodological details of the evaluation provided, and the tools used for data collection and analysis. We discuss the potential benefits, but also the issues of an open science, and conclude with a set of recommendations for implementing open science in TEL.},
annote = {@Nora: OS},
author = {G{\"{u}}, N.A. and Reinhardt, Wolfgang and Leony, Derick and Kraker, Peter and nter Beham},
doi = {10.1504/ijtel.2011.045454},
issn = {1753-5255},
journal = {International Journal of Technology Enhanced Learning},
number = {6},
pages = {643},
title = {{The case for an open science in technology enhanced learning}},
volume = {3},
year = {2012}
}
@manual{Arslan2014,
annote = {R package version 0.8.4},
author = {Arslan, Ruben},
title = {formr: formr survey framework},
year = {2014}
}
@article{Camerer2018,
abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62{\%}) studies, and the effect size of the replications is on average about 50{\%} of the original effect size. Replicability varies between 12 (57{\%}) and 14 (67{\%}) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67{\%} in a Bayesian analysis. The relative effect size of true positives is estimated to be 71{\%}, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-th/9312160v1},
author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck Hua and Huber, J{\"{u}}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric Jan and Wu, Hang},
doi = {10.1038/s41562-018-0399-z},
eprint = {9312160v1},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Camerer et al.{\_}2018.pdf:pdf},
isbn = {2397-3374},
issn = {23973374},
journal = {Nature Human Behaviour},
keywords = {replication},
mendeley-tags = {replication},
number = {9},
pages = {637--644},
primaryClass = {arXiv:hep-th},
title = {{Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015}},
volume = {2},
year = {2018}
}
@article{Edwards2017,
abstract = {Abstract Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics...},
author = {Edwards, Marc A. and Roy, Siddhartha},
doi = {10.1089/ees.2016.0223},
issn = {1557-9018},
journal = {Environmental Engineering Science},
keywords = {academic research,funding,misconduct,perverse incentives,scientific integrity},
month = {jan},
number = {1},
pages = {51--61},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{Academic Research in the 21st Century: Maintaining Scientific Integrity in a Climate of Perverse Incentives and Hypercompetition}},
url = {http://www.liebertpub.com/doi/10.1089/ees.2016.0223},
volume = {34},
year = {2017}
}
@article{Ulrich2016,
author = {Ulrich, Rolf and Erdfelder, Edgar and Deutsch, Roland and Strau{\ss}, Bernhard and Br{\"{u}}ggemann, Anne and Hannover, Bettina and Tuschen-Caffier, Brunna and Kirschbaum, Clemens and Blickle, Gerhard and M{\"{o}}ller, Jens and Rief, Winfried},
doi = {10.1026/0033-3042/a000296},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jul},
number = {3},
pages = {163--174},
title = {{Inflation von falsch-positiven Befunden in der psychologischen Forschung}},
url = {https://econtent.hogrefe.com/doi/10.1026/0033-3042/a000296},
volume = {67},
year = {2016}
}
@article{Chopik2018,
abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining ho...},
author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
doi = {10.1177/0098628318762900},
issn = {0098-6283},
journal = {Teaching of Psychology},
keywords = {best practices,open-source materials,questionable research practices,replication,undergraduate pedagogy},
month = {apr},
number = {2},
pages = {158--163},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{How (and Whether) to Teach Undergraduates About the Replication Crisis in Psychological Science}},
url = {http://journals.sagepub.com/doi/10.1177/0098628318762900},
volume = {45},
year = {2018}
}
@article{Banks2016,
author = {Banks, George C. and Rogelberg, Steven G. and Woznyj, Haley M. and Landis, Ronald S. and Rupp, Deborah E.},
doi = {10.1007/s10869-016-9456-7},
file = {::},
issn = {0889-3268},
journal = {Journal of Business and Psychology},
keywords = {Ethics,Philosophy of science,Questionable research practices QRPs,Research methodology,Research methods,of science {\'{a}} ethics,questionable research practices qrps,research methodology {\'{a}} philosophy,{\'{a}}},
number = {3},
pages = {323--338},
publisher = {Springer US},
title = {{Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly}},
volume = {31},
year = {2016}
}
@article{Krishna2018a,
abstract = {Although questionable research practices (QRPs) and p-hacking have received attention in recent years, little research has focused on their prevalence and acceptance in students. Students are the researchers of the future and will represent the field in the future. Therefore, they should not be learning to use and accept QRPs, which would reduce their ability to produce and evaluate meaningful research. 207 psychology students and fresh graduates provided self-report data on the prevalence and predictors of QRPs. Attitudes towards QRPs, belief that significant results constitute better science or lead to better grades, motivation, and stress levels were predictors. Furthermore, we assessed perceived supervisor attitudes towards QRPs as an important predictive factor. The results were in line with estimates of QRP prevalence from academia. The best predictor of QRP use was students' QRP attitudes. Perceived supervisor attitudes exerted both a direct and indirect effect via student attitudes. Motivation to write a good thesis was a protective factor, whereas stress had no effect. Students in this sample did not subscribe to beliefs that significant results were better for science or their grades. Such beliefs further did not impact QRP attitudes or use in this sample. Finally, students engaged in more QRPs pertaining to reporting and analysis than those pertaining to study design. We conclude that supervisors have an important function in shaping students' attitudes towards QRPs and can improve their research practices by motivating them well. Furthermore, this research provides some impetus towards identifying predictors of QRP use in academia.},
author = {Krishna, Anand and Peter, Sebastian M.},
doi = {10.1371/journal.pone.0203470},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Krishna, Peter{\_}2018.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
title = {{Questionable research practices in student final theses – Prevalence, attitudes, and the role of the supervisor's perceived attitudes}},
volume = {13},
year = {2018}
}
@article{Wicherts2016a,
abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
author = {Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Augusteijn, Hilde E.M. and Bakker, Marjan and van Aert, Robbie C.M. and van Assen, Marcel A.L.M.},
doi = {10.3389/fpsyg.2016.01832},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Wicherts et al.{\_}2016.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Bias,Experimental design,P-hacking,Questionable research practices,Research methods education,Significance chasing,Significance testing},
number = {NOV},
pages = {1--12},
title = {{Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid P-hacking}},
volume = {7},
year = {2016}
}
@misc{Barr2019,
abstract = {At Glasgow our PsyTeachR teaching team has successfully made the transition to teaching R across all undergraduate and postgraduate levels. In this workshop we will share our experiences, insights, and teaching materials. Our curriculum now emphasises certain essential ‘data science' graduate skills that have been overlooked in traditional approaches to teaching, including programming skills, data visualisation, data wrangling and reproducible reports. Students learn about probability and inference through data simulation as well as by working with real datasets. We have also instituted regular coursework across all levels, so that students build their skills through practical experience. We will discuss the challenges associated with implementing this new approach, including staff re-training, supporting student learning through online help sessions, and computer-assisted assessment to handle large volumes of coursework. The workshop is aimed at those interested in making the transition to teaching reproducible research in R at their own institutions, although we also would welcome participation from institutions where R is already a core part of the curriculum.},
author = {Barr, Dale and Woods, Heather Cleland and DeBruine, Lisa and Lai, Rebecca and McAleer, Phil and McNee, Shanon and Nordmann, Emily and Paterson, Helena and Stack, Niamh},
title = {{Redesigning methods curricula for reproducibility}},
url = {https://psyteachr.github.io/sips2019/},
urldate = {2019-08-30},
year = {2019}
}
@article{Schonbrodt2018,
abstract = {Zusammenfassung. Gro{\ss} angelegte Replikationsprojekte der letzten Jahre legen ein aus unserer Sicht beunruhigendes Ausma{\ss} an nicht-replizierbaren Befunden in der wissenschaftlichen Literatur nahe, sowohl in der Psychologie als auch in anderen Disziplinen. Basierend auf einer Analyse einiger Ursachen dieser Situation argumentieren wir, dass der Wandel hin zu einer offenen Wissenschaft (?Open Science?) eine Konsequenz aus der Glaubw{\"{u}}rdigkeitskrise sein muss. Wir pl{\"{a}}dieren f{\"{u}}r konkrete und machbare {\"{A}}nderungen in den Arbeitseinheiten und Instituten vor Ort, und zeigen exemplarisch, welche Schritte am Department Psychologie der Ludwig-Maximilians-Universit{\"{a}}t M{\"{u}}nchen umgesetzt wurden. Diese Schritte betreffen Anreizstrukturen, die Forschungskultur, die Lehre und die Verzahnung mit der Ethikkommission. Sie haben das Ziel, eine reproduzierbarere und glaubw{\"{u}}rdigere Forschung zu unterst{\"{u}}tzen, ohne unn{\"{o}}tige b{\"{u}}rokratische Belastungen zu erzeugen.},
author = {Sch{\"{o}}nbrodt, Felix D. and Maier, Markus and Heene, Moritz and B{\"{u}}hner, Markus},
doi = {10.1026/0033-3042/a000386},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sch{\"{o}}nbrodt et al.{\_}2018.pdf:pdf},
issn = {0033-3042},
journal = {Psychologische Rundschau},
month = {jan},
number = {1},
pages = {37--44},
publisher = {Hogrefe Publishing Group},
title = {{Forschungstransparenz als hohes wissenschaftliches Gut st{\"{a}}rken}},
volume = {69},
year = {2018}
}
@article{Sijtsma2016,
abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices (QRPs). Deliberate or not, QRPs usually have a deteriorating effect on the quality and the credibility of research results. QRPs must be revealed but prevention of QRPs is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one's methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-015-9446-0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sijtsma{\_}2016.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {data fraud,hiring a methodologist/statistician,public availability of data,questionable research practices,replication crisis,solutions},
mendeley-tags = {questionable research practices,replication crisis,solutions},
month = {mar},
number = {1},
pages = {1--15},
pmid = {25820980},
title = {{Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right}},
url = {http://link.springer.com/10.1007/s11336-015-9446-0},
volume = {81},
year = {2016}
}
@article{Matthes2015,
abstract = {Questionable research practices (QRPs) pose a major threat to any scientific discipline. This article analyzes QRPs with a content analysis of more than three decades of published experimental research in four flagship communication journals: Journal of Communication, Communication Research, Journalism {\&} Mass Communication Quarterly,and Media Psychology. Findings reveal indications of small and insufficiently justified sample sizes, a lack of reported effect sizes, an indiscriminate removal of cases and items, an increasing inflation of p-values directly below p {\textless} .05, and a rising share of verified (as opposed to falsified) hypotheses. Implications for authors, reviewers, and editors are discussed.},
author = {Matthes, J{\"{o}}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'{e}}e and Adam, Karoline},
doi = {10.1080/19312458.2015.1096334},
file = {::},
issn = {19312466},
journal = {Communication Methods and Measures},
number = {4},
pages = {193--207},
title = {{Questionable Research Practices in Experimental Communication Research: A Systematic Analysis From 1980 to 2013}},
volume = {9},
year = {2015}
}
@article{Bates2015,
author = {Bates, Douglas and M{\"{a}}chler, Martin and Bolker, Ben and Walker, Steve},
doi = {10.18637/jss.v067.i01},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--48},
title = {{Fitting Linear Mixed-Effects Models Using {\{}lme4{\}}}},
volume = {67},
year = {2015}
}
@article{Bakker2012,
abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96{\%} of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {\textless} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
archivePrefix = {arXiv},
arxivId = {hep-ph/0006269v1},
author = {Bakker, Marjan and van Dijk, Annette and Wicherts, Jelte M.},
doi = {10.1177/1745691612459060},
eprint = {0006269v1},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Bakker, van Dijk, Wicherts{\_}2012.pdf:pdf},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {false positives,p hacking,power,publication bias,replication,replication crisis,sample size},
mendeley-tags = {p hacking,publication bias,replication crisis},
number = {6},
pages = {543--554},
pmid = {26168111},
primaryClass = {hep-ph},
title = {{The Rules of the Game Called Psychological Science}},
volume = {7},
year = {2012}
}
@article{Chambers2013,
author = {Chambers, Christopher D.},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Chambers{\_}2013.pdf:pdf},
journal = {Cortex},
pages = {609--610},
title = {{Registered Reports: A new publishing initiative at Cortex}},
volume = {49},
year = {2013}
}
@article{Tackett2017,
abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has...},
author = {Tackett, Jennifer L. and Lilienfeld, Scott O. and Patrick, Christopher J. and Johnson, Sheri L. and Krueger, Robert F. and Miller, Joshua D. and Oltmanns, Thomas F. and Shrout, Patrick E.},
doi = {10.1177/1745691617690042},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {assessment,diagnosis,disorders,scientific methodology},
month = {sep},
number = {5},
pages = {742--756},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{It's Time to Broaden the Replicability Conversation: Thoughts for and From Clinical Psychological Science}},
url = {http://journals.sagepub.com/doi/10.1177/1745691617690042},
volume = {12},
year = {2017}
}
@manual{RCoreTeam2019,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2019}
}
@article{Agnoli2017a,
abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88{\%}) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L.S. and Albiero, Paolo and Cubelli, Roberto},
doi = {10.1371/journal.pone.0172792},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Agnoli et al.{\_}2017.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pmid = {28296929},
title = {{Questionable research practices among Italian research psychologists}},
volume = {12},
year = {2017}
}
@manual{Wickham2017,
annote = {R package version 1.2.1},
author = {Wickham, Hadley},
title = {{tidyverse: Easily Install and Load the 'Tidyverse'}},
url = {https://cran.r-project.org/package=tidyverse},
year = {2017}
}
@article{Simmons2011,
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
archivePrefix = {arXiv},
arxivId = {2021},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
eprint = {2021},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Simmons, Nelson, Simonsohn{\_}2011.pdf:pdf},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,false positives,methodology,methods,motivated reasoning,p hacking,publication,replication crisis},
mendeley-tags = {false positives,methods,p hacking,replication crisis},
number = {11},
pages = {1359--1366},
pmid = {22006061},
title = {{False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant}},
volume = {22},
year = {2011}
}
@article{Glockner2018,
abstract = {Zusammenfassung. Die Replikationskrise innerhalb der Psychologie hat eine Diskussion {\"{u}}ber g{\"{a}}ngige Praktiken im Forschungsprozess und die beteiligten Institutionen ausgel{\"{o}}st. Wir stellen Ma{\ss}nahmen v...},
author = {Gl{\"{o}}ckner, Andreas and Fiedler, Susann and Renkewitz, Frank},
doi = {10.1026/0033-3042/a000384},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Gl{\"{o}}ckner, Fiedler, Renkewitz{\_}2018.pdf:pdf},
issn = {0033-3042},
journal = {Psychologische Rundschau},
keywords = {der herausforderung,erkenntnisfort-,methodology,open science,schritts und zur vermeidung,sich jede wissenschaftliche disziplin,theory databases,theory specification,von fehlentwicklungen muss,zur gew{\"{a}}hrleistung eines effizienten},
number = {1},
pages = {22--36},
title = {{Belastbare und effiziente Wissenschaft}},
volume = {69},
year = {2018}
}
@article{Sijtsma2016a,
abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices (QRPs). Deliberate or not, QRPs usually have a deteriorating effect on the quality and the credibility of research results. QRPs must be revealed but prevention of QRPs is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one's methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-015-9446-0},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/Sijtsma{\_}2016.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {data fraud,hiring a methodologist/statistician,public availability of data,questionable research practices},
number = {1},
pages = {1--15},
pmid = {25820980},
title = {{Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right}},
volume = {81},
year = {2016}
}
@article{Abston2016,
author = {Abston, Kristie A. and White, Charles D. and Bennett, Andrew A. and Batchelor, John H. and Adkins, Cheryl L. and Banks, George C. and Whelpley, Christopher E. and Pollack, Jeffrey M. and O'Boyle, Ernest H.},
doi = {10.1177/0149206315619011},
file = {::},
issn = {0149-2063},
journal = {Journal of Management},
number = {1},
pages = {5--20},
title = {{Questions About Questionable Research Practices in the Field of Management}},
volume = {42},
year = {2016}
}
@article{John2012,
abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm. Keywords},
annote = {@Nora: QRP, RK, OS},
author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
doi = {10.1177/0956797611430953},
file = {:Users/jobrachem/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Uni/literature/pdfs/John, Loewenstein, Prelec{\_}2012.pdf:pdf},
isbn = {0956-7976},
journal = {Psychological Science},
number = {5},
pages = {524--532},
title = {{Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}},
url = {https://doi.org/10.1177/0956797611430953},
volume = {23},
year = {2012}
}
@article{OpenScienceCollaboration2015,
abstract = {INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {::},
journal = {Science},
number = {6251},
pages = {aac4716},
title = {{Estimating the reproducibility of psychological science}},
url = {http://science.sciencemag.org/content/349/6251/aac4716?casa{\_}token=7AWGYeOunWQAAAAA:7pBsRnNCE5IVS8ECLfGRYsgcGEV{\_}7xXs97it2Y3ihTEsQN9ixtTbBUbsao-RUrucVy4nbqjJHlkvOg},
volume = {349},
year = {2015}
}
